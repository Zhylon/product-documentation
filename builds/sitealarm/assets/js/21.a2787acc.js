(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{295:function(e,t,s){"use strict";s.r(t);var a=s(14),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"robots-txt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#robots-txt"}},[e._v("#")]),e._v(" Robots.txt")]),e._v(" "),t("p",[e._v("Sitealarm scannt Webseiten auf defekte Links und gemischte Inhalte (mixed-content), um dir Bescheid zu geben.\nDu kannst steuern, welche Seiten wir durchsuchen sollen, indem du uns zu deiner "),t("code",[e._v("robots.txt")]),e._v(" Datei hinzufügst.")]),e._v(" "),t("h2",{attrs:{id:"standard-robots-txt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#standard-robots-txt"}},[e._v("#")]),e._v(" Standard robots.txt")]),e._v(" "),t("p",[e._v("Die meisten Webseiten haben eine robots.txt-Datei, die Suchmaschinen und andere Bots darüber informiert, welche Seiten sie durchsuchen dürfen.\nEine typische robots.txt Datei sieht so aus:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("User-agent: *\nDisallow:\n")])])]),t("p",[e._v("Das sagt im Grunde jedem Crawler, der sich an die robots.txt-Spezifikation hält, dass er jede Seite crawlen kann.")]),e._v(" "),t("p",[e._v("Um die Crawler einzuschränken, kannst du die "),t("code",[e._v("Disallow")]),e._v("-Anweisung verwenden, um bestimmte Seiten auszuschließen.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("User-agent: Sitealarm\nDisallow: /admin/\nDisallow: /tmp/\nDisallow: /private/\n")])])]),t("p",[e._v("In diesem Beispiel wird Sitealarm daran gehindert, die Seiten "),t("code",[e._v("/admin/")]),e._v(", "),t("code",[e._v("/tmp/")]),e._v(" und "),t("code",[e._v("/private/")]),e._v(" zu crawlen.")])])}),[],!1,null,null,null);t.default=n.exports}}]);